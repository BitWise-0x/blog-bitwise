---
title: 'Executing ComfyUI Workflows as Standalone Scripts'
date: '2025-11-14'
lastmod: '2025-11-14'
tags: ['ai', 'comfyui', 'python', 'generative-ai', 'automation', 'machine-learning']
draft: false
summary: 'Run ComfyUI workflows programmatically without the web UI. A deep dive into building a standalone Python executor with WorkflowExecutor and ExecutionCache.'
images: ['/static/images/blog/executing-comfyui-workflows/sdxl_workflow.png']
authors: ['default']
canonicalUrl: 'https://blog.bitwisesolutions.co/blog/executing-comfyui-workflows-as-standalone-scripts'
---

## Introduction

<ZoomableImage
  src="/static/images/blog/executing-comfyui-workflows/sdxl_workflow.png"
  alt="ComfyUI SDXL Turbo Workflow"
  width={800}
  height={450}
/>

[ComfyUI](https://github.com/comfyanonymous/ComfyUI) is an open-source graphical user interface for creating and executing complex machine learning workflows. It skyrocketed in popularity alongside the growth of diffusion models by enabling users to design intricate pipelines and experiment with different settings such as text encoding, image generation parameters, and postprocessing. The intuitive drag-and-drop graph-based interface makes ComfyUI a power user tool popular among the developer community.

However, the default setup of ComfyUI relies on a web-based interface and a server backend to execute workflows, with both parts of the code very much intertwined. This makes running ComfyUI workflows in a standalone manner particularly difficult and adds additional setup hassle and execution overhead as developers look to deploy their workflows in more lean applications or as part of a headless processing pipeline.

Recently, I was exploring various solutions to run ComfyUI workflows as standalone scripts, but I could not find any suitable workarounds. The closest alternative was [ComfyUI-to-Python-Extension](https://github.com/pydn/ComfyUI-to-Python-Extension), which converts ComfyUI workflows to Python scripts. However, this still requires an intermediate conversion step that would make **dynamic** workflows more difficult to execute.

After a couple of hours of tinkering, I pieced together a solution that allows you to run ComfyUI workflows as standalone scripts. Read on for more implementation details.

## Prerequisites

Refer to the [ComfyUI Github page](https://github.com/comfyanonymous/ComfyUI) for more information on how to install and run the ComfyUI server. The server makes it easy to create and prototype workflows, and I will also be using it to generate the Workflow API JSON file that we will use to run the workflow as a standalone script.

As part of this post, I will also be referencing the [SDXL Turbo workflow](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/) which you can find over here. If you wish to follow along, you can download the model from the official [SDXL Turbo Repository](https://huggingface.co/stabilityai/sdxl-turbo/blob/main/sd_xl_turbo_1.0_fp16.safetensors) and place it in the `models/checkpoints` directory.

## Understanding the Workflow JSON Format

When working with ComfyUI, it's crucial to understand the two different JSON formats used to represent workflows.

### Workflow.json (Frontend Format)

This is probably the more widely used and recognised format. It is required to layout the graph editor in ComfyUI frontend user interface for visual representation and editing of workflows.

Key characteristics:

- Contains node positions, links, and visual metadata
- Each node has a unique visual identifier, often numeric (e.g., "1", "2", "3")
- Links between nodes are represented as explicit connections
- Follows the [Litegraph](https://github.com/jagenjo/litegraph.js) format, which is designed for graph visualization and manipulation

Here's an example of what the `workflow.json` format looks like:

```json
{
  "nodes": [
    {
      "id": 7,
      "type": "CLIPTextEncode",
      "pos": [352, 176],
      "size": [425.27801513671875, 180.6060791015625],
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [
        {
          "name": "clip",
          "type": "CLIP",
          "link": 39
        }
      ],
      "outputs": [
        {
          "name": "CONDITIONING",
          "type": "CONDITIONING",
          "links": [20],
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "CLIPTextEncode"
      },
      "widgets_values": ["text, watermark"]
    }
  ],
  "links": [
    [18, 14, 0, 13, 3, "SAMPLER"],
    [19, 6, 0, 13, 1, "CONDITIONING"]
  ]
}
```

### Workflow API JSON (Backend Format)

The workflow API JSON can be accessed by switching to `dev` mode and exporting the workflow as an API JSON. Internally, it is also used in the execution of a workflow.

<Image
  src="/static/images/blog/executing-comfyui-workflows/comfyui_api_export.png"
  alt="Exporting workflow as API JSON from ComfyUI dev mode"
  width={800}
  height={450}
/>

Key characteristics:

- Provides a more compact representation optimized for execution
- Eliminates visual/UI-specific metadata such as node positions and sizes
- Links between nodes are embedded directly within the node inputs
- Each node is identified by a unique key (e.g., "5", "6", "7")

```json
{
  "7": {
    "inputs": {
      "text": "text, watermark",
      "clip": ["20", 1]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "20": {
    "inputs": {
      "ckpt_name": "sd_xl_turbo_1.0_fp16.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  }
}
```

The workflow API JSON format serves as the foundation for executing workflows programmatically, as it contains all the necessary information about nodes, their inputs, and the connections between them. Take a closer look at the inputs for node "7" of type "CLIPTextEncode" and you will see that the `inputs` contain properties taken from `widgets_values` in the original `Workflow.json` file as well as any associated links. In this example (`[20, 1]`), the field `clip` is linked to node "20" and the 2nd input slot.[^1]

## Creating a Standalone Execution Script

To execute ComfyUI workflows in a standalone environment, we need to create a Python script that encapsulates the necessary components and logic. There are two main components of the script - `WorkflowExecutor` and `ExecutionCache`, which form the core of the standalone execution process.

The complete standalone execution script is outlined below.

### WorkflowExecutor

The `WorkflowExecutor` is responsible for orchestrating the execution of a ComfyUI workflow. It initializes the execution environment, manages caching, and handles node execution. In the `execute` method, it first validates the workflow JSON, using ComfyUI's `validate_prompt` function before constructing it as a `DynamicPrompt` that gets converted to an `ExecutionList`.

We will then iterate over the `ExecutionList` and execute each node in the workflow using the `_execute_node` method. This relies on ComfyUI's `get_input_data` to retrieve input data for each node and passes it to the `get_output_data` function which executes the node and returns the output data.[^2]

### ExecutionCache

The `ExecutionCache` class provides unified cache management for workflow execution. It handles caching of node outputs, UI data, and object instances, optimizing performance and reducing redundant computations. Under the hood, it is built on top of ComfyUI's `HierarchicalCache` and stores information about a node object, output data, and UI data.

Combining the `WorkflowExecutor` and `ExecutionCache` classes provides a relatively minimal implementation of the logic required to execute workflows as standalone scripts while maintaining as close compatibility with ComfyUI as possible.

### Full Source Code

```python
import json
import logging
import sys
import torch
from typing import Dict, Any, Optional, List, Set

from nodes import init_extra_nodes, NODE_CLASS_MAPPINGS
from comfy_execution.graph import DynamicPrompt, ExecutionList
from comfy_execution.caching import HierarchicalCache, CacheKeySetInputSignature, CacheKeySetID
from execution import validate_prompt, get_input_data, get_output_data, _map_node_over_list, ExecutionBlocker


def setup_comfyui_env():
    """Setup ComfyUI environment and initialize nodes"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        force=True,
        handlers=[
            logging.FileHandler('comfyui_execution.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    init_extra_nodes()


class IsChangedCache:
    """Cache for tracking node changes"""
    def __init__(self, dynprompt, outputs_cache):
        self.dynprompt = dynprompt
        self.outputs_cache = outputs_cache
        self.is_changed = {}

    def get(self, node_id: str) -> Any:
        """Get change status for a node"""
        if node_id in self.is_changed:
            return self.is_changed[node_id]

        node = self.dynprompt.get_node(node_id)
        class_type = node["class_type"]
        class_def = NODE_CLASS_MAPPINGS[class_type]

        if not hasattr(class_def, "IS_CHANGED"):
            self.is_changed[node_id] = False
            return False

        if "is_changed" in node:
            self.is_changed[node_id] = node["is_changed"]
            return node["is_changed"]

        input_data_all, _ = get_input_data(node["inputs"], class_def, node_id, None)
        try:
            is_changed = _map_node_over_list(class_def, input_data_all, "IS_CHANGED")
            node["is_changed"] = [None if isinstance(x, ExecutionBlocker) else x for x in is_changed]
        except Exception as e:
            logging.warning(f"WARNING: {e}")
            node["is_changed"] = float("NaN")
        finally:
            self.is_changed[node_id] = node["is_changed"]
            return self.is_changed[node_id]


class ExecutionCache:
    """Cache management for workflow execution"""
    def __init__(self):
        self.outputs = HierarchicalCache(CacheKeySetInputSignature)
        self.ui = HierarchicalCache(CacheKeySetInputSignature)
        self.objects = HierarchicalCache(CacheKeySetID)
        self._caches = [self.outputs, self.ui, self.objects]

    def initialize(self, prompt: Dict, dynamic_prompt: DynamicPrompt) -> None:
        """Initialize all caches with the prompt"""
        is_changed_cache = IsChangedCache(dynamic_prompt, self.outputs)
        for cache in self._caches:
            cache.set_prompt(dynamic_prompt, set(prompt.keys()), is_changed_cache)
            cache.clean_unused()

    def get_output(self, node_id: str) -> Optional[Any]:
        """Get cached output for a node"""
        return self.outputs.get(node_id)

    def set_output(self, node_id: str, output: Any, ui_data: Optional[Dict] = None) -> None:
        """Set output and UI data for a node"""
        self.outputs.set(node_id, output)
        if ui_data:
            self.ui.set(node_id, {
                "meta": {"node_id": node_id},
                "output": ui_data
            })

    def get_object(self, node_id: str) -> Optional[Any]:
        """Get cached object instance"""
        return self.objects.get(node_id)

    def set_object(self, node_id: str, obj: Any) -> None:
        """Cache object instance"""
        self.objects.set(node_id, obj)


class WorkflowExecutor:
    def __init__(self):
        self.cache = ExecutionCache()
        self.executed: Set[str] = set()
        self.execution_order: List[str] = []

    def _execute_node(self, node_id: str, dynamic_prompt: DynamicPrompt,
                      execution_list: ExecutionList) -> Optional[Any]:
        """Execute a single node in the workflow"""
        if node_id in self.executed:
            return self.cache.get_output(node_id)

        try:
            node = dynamic_prompt.get_node(node_id)
            class_type = node['class_type']
            class_def = NODE_CLASS_MAPPINGS[class_type]

            # Get or create node instance
            node_instance = self.cache.get_object(node_id)
            if node_instance is None:
                node_instance = class_def()
                self.cache.set_object(node_id, node_instance)

            logging.info(f"Executing node {node_id} ({class_type})")

            # Get input data
            input_data_all, missing = get_input_data(
                node['inputs'],
                class_def,
                node_id,
                self.cache.outputs,
                dynprompt=dynamic_prompt,
                extra_data={}
            )

            # Handle lazy inputs
            if hasattr(node_instance, "check_lazy_status"):
                required_inputs = _map_node_over_list(
                    node_instance, input_data_all, "check_lazy_status", allow_interrupt=True
                )
                required_inputs = set(sum([r for r in required_inputs if isinstance(r, list)], []))
                required_inputs = [x for x in required_inputs if isinstance(x, str) and (
                    x not in input_data_all or x in missing
                )]
                if required_inputs:
                    for input_name in required_inputs:
                        execution_list.make_input_strong_link(node_id, input_name)
                    return None

            # Execute node function
            output_data, ui_data, has_subgraph = get_output_data(
                node_instance,
                input_data_all,
                execution_block_cb=None
            )

            if has_subgraph:
                raise ValueError("Subgraph execution not supported in standalone mode")

            # Cache results
            self.cache.set_output(node_id, output_data, ui_data)
            logging.info(f"Executed node {node_id} ({class_type})")
            return output_data

        except Exception as ex:
            logging.error(f"Error executing node {node_id} ({class_type}): {str(ex)}")
            raise

    def execute(self, workflow_json: str | dict) -> Dict[str, Any]:
        """Execute a complete workflow from JSON"""
        prompt = json.loads(workflow_json) if isinstance(workflow_json, str) else workflow_json

        # Validate workflow
        valid, error, outputs, _ = validate_prompt(prompt)
        if not valid:
            raise ValueError(f"Invalid workflow: {error['message']}\n{error['details']}")
        if not outputs:
            raise ValueError("No output nodes found in workflow")

        # Initialize execution
        dynamic_prompt = DynamicPrompt(prompt)
        self.cache.initialize(prompt, dynamic_prompt)
        execution_list = ExecutionList(dynamic_prompt, self.cache.outputs)

        # Add output nodes to execution list
        for node_id in outputs:
            execution_list.add_node(node_id)

        # Execute workflow
        with torch.inference_mode():
            while not execution_list.is_empty():
                node_id, error, _ = execution_list.stage_node_execution()
                if error:
                    raise RuntimeError(f"Execution error: {error['exception_message']}")

                output = self._execute_node(node_id, dynamic_prompt, execution_list)
                if output is None:
                    execution_list.unstage_node_execution()
                else:
                    if node_id not in self.executed:
                        self.execution_order.append(node_id)
                        self.executed.add(node_id)
                    execution_list.complete_node_execution()

        return {
            'outputs': self.cache.outputs.recursive_debug_dump(),
            'executed_nodes': self.execution_order
        }


def execute_workflow(workflow_json: str | dict) -> Dict[str, Any]:
    """Helper function to execute a workflow"""
    executor = WorkflowExecutor()
    return executor.execute(workflow_json)
```

## Demo: Executing the SDXL Turbo Workflow

Now that we have a standalone execution script ready, let's walk through the process of converting the SD Turbo workflow into a standalone execution script.

First, we need to convert the SD Turbo workflow to the API format. You can do this from the user interface in `dev` mode by exporting the workflow as an API JSON from ComfyUI and saving it as `sdxl_workflow_api.json`.

Next, we will create a Python script that reads the workflow JSON, executes the workflow, and prints the executed nodes. Here is a simple example of how you can execute the SDXL Turbo workflow using the standalone execution script:

```python
if __name__ == "__main__":
    setup_comfyui_env()

    workflow_path = "./user/default/workflows/sdxl_workflow_api.json"

    with open(workflow_path) as json_data:
        workflow_json = json.load(json_data)

    results = execute_workflow(workflow_json)
    print("Workflow executed successfully")
    print("Executed nodes: %s", results['executed_nodes'])
```

If all goes well, you should be able to get a generated image in the `output` directory!

<Image
  src="/static/images/blog/executing-comfyui-workflows/ComfyUI_00001.png"
  alt="SDXL Turbo generated image output"
  width={512}
  height={512}
/>

I hope this guide makes working with ComfyUI workflows a little easier and more pleasant.

---

[^1]: While it is also possible to write a script to convert the `Workflow.json` programmatically, there are some challenges in doing so, since a lot of the logic is tied to the ComfyUI frontend. For simplicity, I recommend using the API JSON format.

[^2]: `get_output_data` returns the execution result, UI related output and additional information on whether there is a subgraph.
